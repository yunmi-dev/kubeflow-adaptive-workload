"""
ë¶„ì‚° í•™ìŠµ ì‹œë®¬ë ˆì´í„° (ì´ê¸°ì¢… í™˜ê²½)
- ì‹¤ì œ í•™ìŠµ ìˆ˜í–‰ í›„ ì„±ëŠ¥ ê³„ìˆ˜ë¡œ ì‹œê°„ ìŠ¤ì¼€ì¼ë§
"""
import time
import numpy as np
import tensorflow as tf
from tensorflow import keras
from typing import List, Dict, Tuple
import json

class NodeProfile:
    """ë…¸ë“œ í”„ë¡œí•„ (ì´ê¸°ì¢… í•˜ë“œì›¨ì–´ ì‹œë®¬ë ˆì´ì…˜)"""
    def __init__(self, node_id: int, cpu_factor: float, memory_mb: int, name: str):
        self.node_id = node_id
        self.cpu_factor = cpu_factor  # 1.0 = ê³ ì„±ëŠ¥, 0.5 = ì¤‘ì„±ëŠ¥, 0.25 = ì €ì„±ëŠ¥
        self.memory_mb = memory_mb
        self.name = name
    
    def __repr__(self):
        return f"Node{self.node_id}({self.name}, CPU={self.cpu_factor*100}%, MEM={self.memory_mb}MB)"

class DistributedTrainer:
    def __init__(self, model_config: Dict):
        self.model_config = model_config
        self.batch_size = model_config.get('batch_size', 32)
        self.epochs = model_config.get('epochs', 5)
        
        # ì´ê¸°ì¢… ë…¸ë“œ í”„ë¡œí•„ ì •ì˜
        self.nodes = [
            NodeProfile(1, cpu_factor=1.0, memory_mb=8192, name="High-Perf"),
            NodeProfile(2, cpu_factor=0.5, memory_mb=4096, name="Mid-Perf"),
            NodeProfile(3, cpu_factor=0.25, memory_mb=2048, name="Low-Perf")
        ]
    
    def create_model(self) -> keras.Model:
        """
        ê°„ë‹¨í•œ CNN ëª¨ë¸ (MNIST)
        """
        model = keras.Sequential([
            keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
            keras.layers.MaxPooling2D((2, 2)),
            keras.layers.Conv2D(64, (3, 3), activation='relu'),
            keras.layers.MaxPooling2D((2, 2)),
            keras.layers.Flatten(),
            keras.layers.Dense(64, activation='relu'),
            keras.layers.Dropout(0.5),
            keras.layers.Dense(10, activation='softmax')
        ])
        
        model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def train_on_node(
        self, 
        node: NodeProfile,
        x_train: np.ndarray,
        y_train: np.ndarray,
        verbose: int = 0
    ) -> Tuple[float, float, Dict]:
        """
        íŠ¹ì • ë…¸ë“œì—ì„œ í•™ìŠµ ìˆ˜í–‰
        
        Returns:
            actual_time: ì‹¤ì œ ê±¸ë¦° ì‹œê°„
            simulated_time: ì„±ëŠ¥ ê³„ìˆ˜ ì ìš©í•œ ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„
            metrics: í•™ìŠµ ë©”íŠ¸ë¦­
        """
        print(f"\n{'='*60}")
        print(f"[{node.name}] Node {node.node_id} í•™ìŠµ ì‹œì‘")
        print(f"  - í• ë‹¹ ë°ì´í„°: {len(x_train):,} samples")
        print(f"  - CPU ì„±ëŠ¥: {node.cpu_factor*100:.0f}% (ê¸°ì¤€ ëŒ€ë¹„)")
        print(f"  - ë©”ëª¨ë¦¬: {node.memory_mb} MB")
        print(f"{'='*60}")
        
        # ëª¨ë¸ ìƒì„±
        model = self.create_model()
        
        # ì‹¤ì œ í•™ìŠµ ìˆ˜í–‰
        start_time = time.time()
        
        history = model.fit(
            x_train, y_train,
            batch_size=self.batch_size,
            epochs=self.epochs,
            verbose=verbose,
            validation_split=0.1
        )
        
        actual_time = time.time() - start_time
        
        # ì„±ëŠ¥ ê³„ìˆ˜ ì ìš©: ëŠë¦° ë…¸ë“œëŠ” ê°™ì€ ì‘ì—…ì„ ë” ì˜¤ë˜ ìˆ˜í–‰
        # ì‹¤ì œ í•˜ë“œì›¨ì–´ë¼ë©´ CPUê°€ ëŠë ¤ì„œ ì‹œê°„ì´ ë” ê±¸ë¦¼
        simulated_time = actual_time / node.cpu_factor
        
        # í•™ìŠµ ë©”íŠ¸ë¦­
        final_loss = history.history['loss'][-1]
        final_acc = history.history['accuracy'][-1]
        val_loss = history.history['val_loss'][-1]
        val_acc = history.history['val_accuracy'][-1]
        
        metrics = {
            'node_id': node.node_id,
            'node_name': node.name,
            'cpu_factor': node.cpu_factor,
            'data_size': len(x_train),
            'actual_time': actual_time,
            'simulated_time': simulated_time,
            'final_loss': final_loss,
            'final_accuracy': final_acc,
            'val_loss': val_loss,
            'val_accuracy': val_acc,
            'samples_per_sec': len(x_train) / simulated_time
        }
        
        print(f"\n[{node.name}] í•™ìŠµ ì™„ë£Œ")
        print(f"  - ì‹¤ì œ ì‹œê°„: {actual_time:.2f}s")
        print(f"  - ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„: {simulated_time:.2f}s (CPU {node.cpu_factor*100:.0f}% ë°˜ì˜)")
        print(f"  - ìµœì¢… ì •í™•ë„: {final_acc:.4f}")
        print(f"  - ì²˜ë¦¬ ì†ë„: {metrics['samples_per_sec']:.2f} samples/sec")
        
        return actual_time, simulated_time, metrics
    
    def run_distributed_training(
        self,
        data_distributions: List[int],
        experiment_name: str = "experiment"
    ) -> Dict:
        """
        ë¶„ì‚° í•™ìŠµ ì‹¤í—˜ ì‹¤í–‰
        
        Args:
            data_distributions: ê° ë…¸ë“œì— í• ë‹¹í•  ë°ì´í„° ê°œìˆ˜ [node1, node2, node3]
            experiment_name: ì‹¤í—˜ ì´ë¦„
        
        Returns:
            ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n{'#'*60}")
        print(f"# ì‹¤í—˜ ì‹œì‘: {experiment_name}")
        print(f"# ë°ì´í„° ë¶„ë°°: {data_distributions}")
        print(f"{'#'*60}")
        
        # MNIST ë°ì´í„° ë¡œë“œ
        (x_train_full, y_train_full), (x_test, y_test) = keras.datasets.mnist.load_data()
        x_train_full = x_train_full.reshape(-1, 28, 28, 1) / 255.0
        x_test = x_test.reshape(-1, 28, 28, 1) / 255.0
        
        # ë°ì´í„° ë¶„í• 
        start_idx = 0
        node_data = []
        for dist in data_distributions:
            end_idx = start_idx + dist
            node_data.append((
                x_train_full[start_idx:end_idx],
                y_train_full[start_idx:end_idx]
            ))
            start_idx = end_idx
        
        # ê° ë…¸ë“œì—ì„œ í•™ìŠµ (ì‹¤ì œë¡œëŠ” ë³‘ë ¬ì´ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ìˆœì°¨)
        all_metrics = []
        actual_times = []
        simulated_times = []
        
        for i, node in enumerate(self.nodes):
            x_train, y_train = node_data[i]
            
            actual_t, sim_t, metrics = self.train_on_node(
                node, x_train, y_train, verbose=0
            )
            
            actual_times.append(actual_t)
            simulated_times.append(sim_t)
            all_metrics.append(metrics)
        
        # JCT ê³„ì‚° (ê°€ì¥ ëŠë¦° ë…¸ë“œê°€ ì „ì²´ ì‹œê°„ ê²°ì •)
        jct_actual = max(actual_times)
        jct_simulated = max(simulated_times)
        
        # ì‹œê°„ ê· í˜•ë„ ê³„ì‚°
        balance_metrics = self._calculate_balance(simulated_times)
        
        # ê²°ê³¼ ì •ë¦¬
        result = {
            'experiment_name': experiment_name,
            'data_distributions': data_distributions,
            'nodes': [n.__dict__ for n in self.nodes],
            'node_metrics': all_metrics,
            'jct_actual': jct_actual,
            'jct_simulated': jct_simulated,
            'actual_times': actual_times,
            'simulated_times': simulated_times,
            'balance': balance_metrics,
            'total_data': sum(data_distributions)
        }
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_summary(result)
        
        return result
    
    def _calculate_balance(self, times: List[float]) -> Dict[str, float]:
        """ì‹œê°„ ê· í˜•ë„ ê³„ì‚°"""
        times_arr = np.array(times)
        mean_time = np.mean(times_arr)
        std_time = np.std(times_arr)
        
        balance = 1 - (std_time / mean_time) if mean_time > 0 else 0
        
        return {
            'mean_time': float(mean_time),
            'std_time': float(std_time),
            'balance_score': float(balance),
            'min_time': float(np.min(times_arr)),
            'max_time': float(np.max(times_arr))
        }
    
    def _print_summary(self, result: Dict):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"ì‹¤í—˜ ê²°ê³¼ ìš”ì•½: {result['experiment_name']}")
        print(f"{'='*60}")
        
        print(f"\nğŸ“Š ë°ì´í„° ë¶„ë°°:")
        for i, dist in enumerate(result['data_distributions']):
            ratio = dist / result['total_data'] * 100
            print(f"  Node {i+1}: {dist:,} samples ({ratio:.1f}%)")
        
        print(f"\nâ±ï¸  í•™ìŠµ ì‹œê°„ (ì‹œë®¬ë ˆì´ì…˜):")
        for i, t in enumerate(result['simulated_times']):
            print(f"  Node {i+1}: {t:.2f}s")
        
        print(f"\nğŸ¯ í•µì‹¬ ì§€í‘œ:")
        print(f"  JCT (Job Completion Time): {result['jct_simulated']:.2f}s")
        print(f"  í‰ê·  ì‹œê°„: {result['balance']['mean_time']:.2f}s")
        print(f"  ì‹œê°„ í¸ì°¨ (Ïƒ): {result['balance']['std_time']:.2f}s")
        print(f"  ì‹œê°„ ê· í˜•ë„: {result['balance']['balance_score']:.4f}")
        
        print(f"\nâœ… ëª¨ë¸ ì •í™•ë„:")
        for metric in result['node_metrics']:
            print(f"  Node {metric['node_id']}: {metric['final_accuracy']:.4f}")
        
        print(f"{'='*60}\n")

def compare_baseline_vs_adaptive():
    """Baseline vs Adaptive ë¹„êµ ì‹¤í—˜"""
    
    config = {
        'batch_size': 32,
        'epochs': 3  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
    }
    
    trainer = DistributedTrainer(config)
    
    total_data = 60000
    
    # 1. Baseline: ê· ë“± ë¶„ë°°
    baseline_dist = [20000, 20000, 20000]
    baseline_result = trainer.run_distributed_training(
        baseline_dist,
        experiment_name="Baseline (ê· ë“± ë¶„ë°°)"
    )
    
    # 2. Adaptive: ì„±ëŠ¥ ê¸°ë°˜ ë¶„ë°° (ì„±ëŠ¥ ë¹„ìœ¨ 1.0:0.5:0.25 = 4:2:1)
    # ì´í•© = 7, Node1 = 4/7, Node2 = 2/7, Node3 = 1/7
    adaptive_dist = [
        int(total_data * 4 / 7),  # ~34,286
        int(total_data * 2 / 7),  # ~17,143
        int(total_data * 1 / 7)   # ~8,571
    ]
    # í•©ê³„ ë§ì¶”ê¸°
    adaptive_dist[0] += total_data - sum(adaptive_dist)
    
    adaptive_result = trainer.run_distributed_training(
        adaptive_dist,
        experiment_name="Adaptive (ì„±ëŠ¥ ê¸°ë°˜ ë¶„ë°°)"
    )
    
    # 3. ë¹„êµ ë¶„ì„
    print(f"\n{'#'*60}")
    print("# ìµœì¢… ë¹„êµ ë¶„ì„")
    print(f"{'#'*60}\n")
    
    baseline_jct = baseline_result['jct_simulated']
    adaptive_jct = adaptive_result['jct_simulated']
    improvement = (baseline_jct - adaptive_jct) / baseline_jct * 100
    
    print(f"ğŸ“ˆ JCT ë¹„êµ:")
    print(f"  Baseline:  {baseline_jct:.2f}s")
    print(f"  Adaptive:  {adaptive_jct:.2f}s")
    print(f"  ê°œì„ ìœ¨:    {improvement:.2f}%")
    
    print(f"\nâš–ï¸  ì‹œê°„ ê· í˜•ë„:")
    print(f"  Baseline:  {baseline_result['balance']['balance_score']:.4f}")
    print(f"  Adaptive:  {adaptive_result['balance']['balance_score']:.4f}")
    
    # ê²°ê³¼ ì €ì¥
    comparison = {
        'baseline': baseline_result,
        'adaptive': adaptive_result,
        'improvement_percent': improvement
    }
    
    return comparison

if __name__ == "__main__":
    print("ğŸš€ ë¶„ì‚° í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘\n")
    results = compare_baseline_vs_adaptive()
    
    # JSON ì €ì¥
    import os
    os.makedirs('results', exist_ok=True)
    
    with open('results/baseline_vs_adaptive.json', 'w') as f:
        # numpy íƒ€ì… ë³€í™˜
        def convert(o):
            if isinstance(o, np.integer):
                return int(o)
            if isinstance(o, np.floating):
                return float(o)
            if isinstance(o, np.ndarray):
                return o.tolist()
            return o
        
        json.dump(results, f, indent=2, default=convert)
    
    print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: results/baseline_vs_adaptive.json")